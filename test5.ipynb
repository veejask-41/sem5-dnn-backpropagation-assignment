{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "[[0.75000574 0.74998778 0.7499874  0.75001908]]\n",
      "Epoch: 0, Train Loss: 5.545177297371861, Test Loss: 5.545177297402191, Train Accuracy: 0.25734852017994864, Test Accuracy: 0.2550251691735208\n",
      "[[0.75000574 0.74998778 0.7499874  0.75001908]]\n",
      "[[1.49996208 1.49996136 1.49996376 1.5001128 ]]\n",
      "Epoch: 1, Train Loss: 5.545177390814024, Test Loss: 5.5451773914504265, Train Accuracy: 0.22167851033010744, Test Accuracy: 0.22434276143067353\n",
      "[[1.49996208 1.49996136 1.49996376 1.5001128 ]]\n",
      "[[2.24982796 2.24994708 2.24987332 2.25035164]]\n",
      "Epoch: 2, Train Loss: 5.545177966874004, Test Loss: 5.545177968954516, Train Accuracy: 0.19971890914050686, Test Accuracy: 0.2034757909300034\n",
      "[[2.24982796 2.24994708 2.24987332 2.25035164]]\n",
      "[[2.9995308  2.99988046 2.99968742 3.00090133]]\n",
      "Epoch: 3, Train Loss: 5.545180742216713, Test Loss: 5.545180744358687, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[2.9995308  2.99988046 2.99968742 3.00090133]]\n",
      "[[3.74878263 3.7497577  3.74925887 3.7522008 ]]\n",
      "Epoch: 4, Train Loss: 5.545196919841026, Test Loss: 5.545196921414362, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[3.74878263 3.7497577  3.74925887 3.7522008 ]]\n",
      "[[4.49705142 4.49943758 4.49822883 4.50528217]]\n",
      "Epoch: 5, Train Loss: 5.545287058801378, Test Loss: 5.545287043881181, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[4.49705142 4.49943758 4.49822883 4.50528217]]\n",
      "[[5.24471713 5.24897622 5.2467892  5.25951745]]\n",
      "Epoch: 6, Train Loss: 5.545384111757237, Test Loss: 5.545383991340962, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[5.24471713 5.24897622 5.2467892  5.25951745]]\n",
      "[[6.02341836 6.00615222 6.01536776 5.95506166]]\n",
      "Epoch: 7, Train Loss: 5.576448577123299, Test Loss: 5.576415859507866, Train Accuracy: 0.2758278650999542, Test Accuracy: 0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in exp\n",
      "c:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.02341836 6.00615222 6.01536776 5.95506166]]\n",
      "[[nan nan nan nan]]\n",
      "Epoch: 8, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[nan nan nan nan]]\n",
      "[[nan nan nan nan]]\n",
      "Epoch: 9, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "[[nan nan nan nan]]\n",
      "[[nan nan nan nan]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7745425656b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden1_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden2_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-7745425656b1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_test, y_test, epochs)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;31m# print(\"accuracy computed\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7745425656b1>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Function to compute accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 3373\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   3374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# loading dataset\n",
    "X_train = pd.read_csv(r\"Task_2\\x_train.csv\").values  # features\n",
    "y_train = pd.read_csv(r\"Task_2\\y_train.csv\").values  # labels\n",
    "\n",
    "X_test = pd.read_csv(r\"Task_2\\x_test.csv\").values\n",
    "y_test = pd.read_csv(r\"Task_2\\y_test.csv\").values\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU activation function\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "# softmax activation function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot_encoding(y, num_classes=4):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Function to compute accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "# Neural Network class with backpropagation and gradient descent\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.weights_input_hidden1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.bias_hidden1 = np.zeros((1, hidden1_size))\n",
    "\n",
    "        self.weights_hidden1_hidden2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.bias_hidden2 = np.zeros((1, hidden2_size))\n",
    "\n",
    "        self.weights_hidden2_output = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input to hidden layer 1\n",
    "        # print(X.shape)\n",
    "        z1 = np.dot(X, self.weights_input_hidden1) + self.bias_hidden1\n",
    "        # print(z1.shape)\n",
    "        a1 = relu(z1)\n",
    "        # print(a1.shape)\n",
    "\n",
    "        # Hidden layer 1 to hidden layer 2\n",
    "        z2 = np.dot(a1, self.weights_hidden1_hidden2) + self.bias_hidden2\n",
    "        # print(z2.shape)\n",
    "        a2 = relu(z2)\n",
    "        # print(a2.shape)\n",
    "\n",
    "        # Hidden layer 2 to output\n",
    "        z3 = np.dot(a2, self.weights_hidden2_output) + self.bias_output\n",
    "        # print(z3.shape)\n",
    "        output = softmax(z3)\n",
    "        # print(output.shape)\n",
    "\n",
    "        return output, a2, z2, a1, z1\n",
    "\n",
    "    def backward(self, X, y_true, output, a2, z2, a1, z1):\n",
    "        m = X.shape[0]  # Number of samples\n",
    "\n",
    "        # Compute gradient of output layer (Softmax + Cross-entropy loss)\n",
    "        dz3 = output - y_true\n",
    "        dw3 = np.dot(a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 2\n",
    "        dz2 = np.dot(dz3, self.weights_hidden2_output.T) * relu_derivative(a2)\n",
    "        dw2 = np.dot(a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 1\n",
    "        dz1 = np.dot(dz2, self.weights_hidden1_hidden2.T) * relu_derivative(a1)\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.weights_hidden2_output -= self.learning_rate * dw3\n",
    "        print(self.bias_output)\n",
    "        self.bias_output -= self.learning_rate * db3\n",
    "        print(self.bias_output)\n",
    "\n",
    "        self.weights_hidden1_hidden2 -= self.learning_rate * dw2\n",
    "        self.bias_hidden2 -= self.learning_rate * db2\n",
    "\n",
    "        self.weights_input_hidden1 -= self.learning_rate * dw1\n",
    "        self.bias_hidden1 -= self.learning_rate * db1\n",
    "\n",
    "    # fucntion to train the model and return the loss for train and test \n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs=50):\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        train_accuracy = []\n",
    "        test_accuracy = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred, a2_train, z2_train, a1_train, z1_train = self.forward(X_train)\n",
    "            # print(y_pred.shape)\n",
    "            y_test_pred, _, _, _, _ = self.forward(X_test)\n",
    "            # print(\"forward pass done\")\n",
    "\n",
    "            # one hot \n",
    "            # print(y_train.shape)\n",
    "            one_hot_y_train = one_hot_encoding(y_train)\n",
    "            # print(one_hot_y_train.shape)\n",
    "            one_hot_y_test = one_hot_encoding(y_test)\n",
    "            # print(\"one hot done\")\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(one_hot_y_train, y_pred)\n",
    "            test_loss.append(cross_entropy_loss(one_hot_y_test, y_test_pred))\n",
    "            train_loss.append(loss)\n",
    "            # print(\"loss computed\")\n",
    "\n",
    "            # Backward pass\n",
    "            # print(\"backward pass started\")\n",
    "            # print(one_hot_y_train.shape)\n",
    "            # print(y_pred.shape)\n",
    "            self.backward(X_train, one_hot_y_train, y_pred, a2_train, z2_train, a1_train, z1_train)\n",
    "            # print(\"backward pass done\")\n",
    "\n",
    "            # Compute accuracy\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "            train_accuracy.append(accuracy(y_train, y_pred))\n",
    "            test_accuracy.append(accuracy(y_test, y_test_pred))\n",
    "            # print(\"accuracy computed\")\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                # print loss and accuracy for train and test\n",
    "                # print(train_loss)\n",
    "                # print(test_loss)\n",
    "                # print(train_accuracy)\n",
    "                # print(test_accuracy)\n",
    "                print(f\"Epoch: {i}, Train Loss: {loss}, Test Loss: {test_loss[-1]}, Train Accuracy: {train_accuracy[-1]}, Test Accuracy: {test_accuracy[-1]}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_size = 14\n",
    "    hidden1_size = 100\n",
    "    hidden2_size = 40\n",
    "    output_size = 4\n",
    "    learning_rate = 1\n",
    "\n",
    "    nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, output_size, learning_rate)\n",
    "    nn.train(X_train, y_train, X_test, y_test, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
