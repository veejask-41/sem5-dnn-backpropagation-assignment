{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU activation function\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "# softmax activation function\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, new_y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(new_y_pred + 1e-8)) / m\n",
    "\n",
    "# one hot encoding\n",
    "def one_hot_encoding(y):\n",
    "    one_hot = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Function to compute accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "X_train = pd.read_csv(r\"Task_2\\x_train.csv\").values  # features\n",
    "y_train = pd.read_csv(r\"Task_2\\y_train.csv\").values  # labels\n",
    "\n",
    "X_test = pd.read_csv(r\"Task_2\\x_test.csv\").values\n",
    "y_test = pd.read_csv(r\"Task_2\\y_test.csv\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class with backpropagation and gradient descent\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.weights_input_hidden1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.bias_hidden1 = np.zeros((1, hidden1_size))\n",
    "\n",
    "        self.weights_hidden1_hidden2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.bias_hidden2 = np.zeros((1, hidden2_size))\n",
    "\n",
    "        self.weights_hidden2_output = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input to hidden layer 1\n",
    "        # print(X.shape)\n",
    "        z1 = np.dot(X, self.weights_input_hidden1) + self.bias_hidden1\n",
    "        # print(z1.shape)\n",
    "        a1 = relu(z1)\n",
    "        # print(a1.shape)\n",
    "\n",
    "        # Hidden layer 1 to hidden layer 2\n",
    "        z2 = np.dot(a1, self.weights_hidden1_hidden2) + self.bias_hidden2\n",
    "        # print(z2.shape)\n",
    "        a2 = relu(z2)\n",
    "        # print(a2.shape)\n",
    "\n",
    "        # Hidden layer 2 to output\n",
    "        z3 = np.dot(a2, self.weights_hidden2_output) + self.bias_output\n",
    "        # print(z3.shape)\n",
    "        output = softmax(z3)\n",
    "        # print(output.shape)\n",
    "\n",
    "        return output, a2, z2, a1, z1\n",
    "\n",
    "    def backward(self, X, y_true, output, a2, z2, a1, z1):\n",
    "        m = X.shape[0]  # Number of samples\n",
    "\n",
    "        # Compute gradient of output layer (Softmax + Cross-entropy loss)\n",
    "        dz3 = output - y_true\n",
    "        dw3 = np.dot(a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 2\n",
    "        dz2 = np.dot(dz3, self.weights_hidden2_output.T) * relu_derivative(z2)\n",
    "        dw2 = np.dot(a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 1\n",
    "        dz1 = np.dot(dz2, self.weights_hidden1_hidden2.T) * relu_derivative(z1)\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.weights_hidden2_output -= self.learning_rate * dw3\n",
    "        self.bias_output -= self.learning_rate * db3\n",
    "\n",
    "        self.weights_hidden1_hidden2 -= self.learning_rate * dw2\n",
    "        self.bias_hidden2 -= self.learning_rate * db2\n",
    "\n",
    "        self.weights_input_hidden1 -= self.learning_rate * dw1\n",
    "        self.bias_hidden1 -= self.learning_rate * db1\n",
    "\n",
    "    # fucntion to train the model and return the loss for train and test \n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs=50):\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        train_accuracy = []\n",
    "        test_accuracy = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred, a2_train, z2_train, a1_train, z1_train = self.forward(X_train)\n",
    "            # print(y_pred.shape)\n",
    "            y_test_pred, _, _, _, _ = self.forward(X_test)\n",
    "            # print(\"forward pass done\")\n",
    "\n",
    "            # one hot \n",
    "            # print(y_train.shape)\n",
    "            one_hot_y_train = one_hot_encoding(y_train)\n",
    "            # print(one_hot_y_train.shape)\n",
    "            one_hot_y_test = one_hot_encoding(y_test)\n",
    "            # print(\"one hot done\")\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(one_hot_y_train, y_pred)\n",
    "            test_loss.append(cross_entropy_loss(one_hot_y_test, y_test_pred))\n",
    "            train_loss.append(loss)\n",
    "            # print(\"loss computed\")\n",
    "\n",
    "            # Backward pass\n",
    "            # print(\"backward pass started\")\n",
    "            # print(one_hot_y_train.shape)\n",
    "            # print(y_pred.shape)\n",
    "            self.backward(X_train, one_hot_y_train, y_pred, a2_train, z2_train, a1_train, z1_train)\n",
    "            # print(\"backward pass done\")\n",
    "\n",
    "            # Compute accuracy\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "            train_accuracy.append(accuracy(y_train, y_pred))\n",
    "            test_accuracy.append(accuracy(y_test, y_test_pred))\n",
    "            # print(\"accuracy computed\")\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                # print loss and accuracy for train and test\n",
    "                print(f\"Epoch: {i}, Train Loss: {loss}, Test Loss: {test_loss[-1]}, Train Accuracy: {train_accuracy[-1]}, Test Accuracy: {test_accuracy[-1]}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([0.45, 0.65, 0.34, 0.78])\n",
    "np.argmax(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13106, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 5.545177297371861, Test Loss: 5.545177297402191, Train Accuracy: 0.25734852017994864, Test Accuracy: 0.2550251691735208\n",
      "Epoch: 1, Train Loss: 5.545177390814024, Test Loss: 5.5451773914504265, Train Accuracy: 0.22167851033010744, Test Accuracy: 0.22434276143067353\n",
      "Epoch: 2, Train Loss: 5.545177966874004, Test Loss: 5.545177968954516, Train Accuracy: 0.19971890914050686, Test Accuracy: 0.2034757909300034\n",
      "Epoch: 3, Train Loss: 5.545180742216713, Test Loss: 5.545180744358687, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 4, Train Loss: 5.545196919841026, Test Loss: 5.545196921414362, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 5, Train Loss: 5.545287058801378, Test Loss: 5.545287043881181, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 6, Train Loss: 5.545384111757237, Test Loss: 5.545383991340962, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 7, Train Loss: 5.576448577123299, Test Loss: 5.576415859507866, Train Accuracy: 0.2758278650999542, Test Accuracy: 0.2777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  if sys.path[0] == '':\n",
      "c:\\Users\\sajee\\miniconda3\\envs\\dnn-assignment\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 9, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 10, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 11, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 12, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 13, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 14, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 15, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 16, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 17, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 18, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 19, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 20, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 21, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 22, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 23, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 24, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 25, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 26, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 27, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 28, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 29, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 30, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 31, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 32, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 33, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 34, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 35, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 36, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 37, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 38, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 39, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 40, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 41, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 42, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 43, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 44, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 45, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 46, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 47, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 48, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n",
      "Epoch: 49, Train Loss: nan, Test Loss: nan, Train Accuracy: 0.19800091561117045, Test Accuracy: 0.20238095238095238\n"
     ]
    }
   ],
   "source": [
    "input_size = 14\n",
    "hidden1_size = 100\n",
    "hidden2_size = 40\n",
    "output_size = 4\n",
    "learning_rate = 1\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden1_size, hidden2_size, output_size, learning_rate)\n",
    "nn.train(X_train, y_train, X_test, y_test, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU activation function\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Softmax function with numerical stability\n",
    "def softmax(x):\n",
    "    shift_x = x - np.max(x, axis=1, keepdims=True)  # Subtract max for stability\n",
    "    exp_x = np.exp(shift_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function with stability (prevents log(0))\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    y_pred = np.clip(y_pred, 1e-10, 1.0)  # Clip values to avoid log(0)\n",
    "    correct_confidences = y_pred[range(n_samples), y_true]\n",
    "    loss = -np.sum(np.log(correct_confidences)) / n_samples\n",
    "    return loss\n",
    "\n",
    "# Function to compute accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Backpropagation and weight update with gradient clipping\n",
    "def backprop(X, y, W1, b1, W2, b2, W3, b3, learning_rate, clip_value=5.0):\n",
    "    # Forward propagation\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)  # ReLU activation\n",
    "\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = relu(z2)  # ReLU activation\n",
    "\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    output = softmax(z3)  # Softmax activation\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = cross_entropy_loss(y, output)\n",
    "\n",
    "    # Backward propagation\n",
    "    m = y.shape[0]\n",
    "    output[range(m), y] -= 1\n",
    "    output /= m\n",
    "\n",
    "    dW3 = np.dot(a2.T, output)\n",
    "    db3 = np.sum(output, axis=0, keepdims=True)\n",
    "\n",
    "    da2 = np.dot(output, W3.T)\n",
    "    dz2 = da2 * relu_derivative(z2)  # ReLU derivative\n",
    "\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * relu_derivative(z1)  # ReLU derivative\n",
    "\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    dW3 = np.clip(dW3, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    db3 = np.clip(db3, -clip_value, clip_value)\n",
    "\n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3, loss\n",
    "\n",
    "# Train the neural network\n",
    "def train_neural_network(X_train, y_train, X_test, y_test, learning_rate, epochs):\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size1 = 100\n",
    "    hidden_size2 = 40\n",
    "    output_size = 4  # Number of classes\n",
    "\n",
    "    # Initialize weights and biases with small random values\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size1))\n",
    "    W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "    b2 = np.zeros((1, hidden_size2))\n",
    "    W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "    b3 = np.zeros((1, output_size))\n",
    "\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Backpropagation and weight updates\n",
    "        W1, b1, W2, b2, W3, b3, train_loss = backprop(X_train, y_train, W1, b1, W2, b2, W3, b3, learning_rate)\n",
    "\n",
    "        # Forward pass for training accuracy\n",
    "        z1 = np.dot(X_train, W1) + b1\n",
    "        a1 = relu(z1)  # ReLU activation\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        a2 = relu(z2)  # ReLU activation\n",
    "        z3 = np.dot(a2, W3) + b3\n",
    "        train_output = softmax(z3)\n",
    "        y_train_pred = np.argmax(train_output, axis=1)\n",
    "        train_acc = accuracy(y_train, y_train_pred)\n",
    "\n",
    "        # Forward pass for test accuracy\n",
    "        z1_test = np.dot(X_test, W1) + b1\n",
    "        a1_test = relu(z1_test)  # ReLU activation\n",
    "        z2_test = np.dot(a1_test, W2) + b2\n",
    "        a2_test = relu(z2_test)  # ReLU activation\n",
    "        z3_test = np.dot(a2_test, W3) + b3\n",
    "        test_output = softmax(z3_test)\n",
    "        y_test_pred = np.argmax(test_output, axis=1)\n",
    "        test_acc = accuracy(y_test, y_test_pred)\n",
    "\n",
    "        # Calculate test loss\n",
    "        test_loss = cross_entropy_loss(y_test, test_output)\n",
    "\n",
    "        # Store loss and accuracy values\n",
    "        train_costs.append(train_loss)\n",
    "        test_costs.append(test_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    return train_costs, test_costs, train_accs, test_accs\n",
    "\n",
    "# Plot training results\n",
    "def plot_metrics(train_costs, test_costs, train_accs, test_accs, learning_rate):\n",
    "    epochs = len(train_costs)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), train_costs, label='Train Loss')\n",
    "    plt.plot(range(epochs), test_costs, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss vs Epochs (Learning Rate = {learning_rate})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), train_accs, label='Train Accuracy')\n",
    "    plt.plot(range(epochs), test_accs, label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy vs Epochs (Learning Rate = {learning_rate})')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X_train = pd.read_csv(r\"Task_2\\x_train.csv\").values  # features\n",
    "y_train = pd.read_csv(r\"Task_2\\y_train.csv\").values  # labels\n",
    "\n",
    "X_test = pd.read_csv(r\"Task_2\\x_test.csv\").values\n",
    "y_test = pd.read_csv(r\"Task_2\\y_test.csv\").values\n",
    "\n",
    "\n",
    "learning_rates = [1, 0.1, 0.001]\n",
    "epochs = 50\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    train_costs, test_costs, train_accs, test_accs = train_neural_network(X_train, y_train, X_test, y_test, lr, epochs)\n",
    "    plot_metrics(train_costs, test_costs, train_accs, test_accs, lr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
