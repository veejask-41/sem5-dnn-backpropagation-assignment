{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 1.2995414654035606\n",
      "Epoch 200/1000, Loss: 1.2440732431471346\n",
      "Epoch 300/1000, Loss: 1.2073306817754061\n",
      "Epoch 400/1000, Loss: 1.1818556261554292\n",
      "Epoch 500/1000, Loss: 1.1634265664822026\n",
      "Epoch 600/1000, Loss: 1.1495810392073493\n",
      "Epoch 700/1000, Loss: 1.138801922226387\n",
      "Epoch 800/1000, Loss: 1.1301310937694344\n",
      "Epoch 900/1000, Loss: 1.122936027495496\n",
      "Epoch 1000/1000, Loss: 1.1167728951532916\n",
      "\n",
      "Predicted Probabilities:\n",
      "    Class_0   Class_1   Class_2   Class_3\n",
      "0  0.374872  0.061664  0.376161  0.187303\n",
      "1  0.374468  0.061598  0.376819  0.187116\n",
      "2  0.375829  0.061723  0.375026  0.187421\n",
      "3  0.375613  0.061686  0.375376  0.187324\n",
      "4  0.375180  0.061741  0.375214  0.187865\n"
     ]
    }
   ],
   "source": [
    "# Define ReLU activation function and its derivative\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Define Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "# One-hot encoding for labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Neural Network class with backpropagation and gradient descent\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights and biases\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        self.weights_input_hidden1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.bias_hidden1 = np.zeros((1, hidden1_size))\n",
    "\n",
    "        self.weights_hidden1_hidden2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.bias_hidden2 = np.zeros((1, hidden2_size))\n",
    "\n",
    "        self.weights_hidden2_output = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input to hidden layer 1\n",
    "        self.z1 = np.dot(X, self.weights_input_hidden1) + self.bias_hidden1\n",
    "        self.a1 = relu(self.z1)\n",
    "\n",
    "        # Hidden layer 1 to hidden layer 2\n",
    "        self.z2 = np.dot(self.a1, self.weights_hidden1_hidden2) + self.bias_hidden2\n",
    "        self.a2 = relu(self.z2)\n",
    "\n",
    "        # Hidden layer 2 to output\n",
    "        self.z3 = np.dot(self.a2, self.weights_hidden2_output) + self.bias_output\n",
    "        self.output = softmax(self.z3)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, X, y_true):\n",
    "        m = X.shape[0]  # Number of samples\n",
    "\n",
    "        # Compute gradient of output layer (Softmax + Cross-entropy loss)\n",
    "        dz3 = self.output - y_true\n",
    "        dw3 = np.dot(self.a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 2\n",
    "        dz2 = np.dot(dz3, self.weights_hidden2_output.T) * relu_derivative(self.z2)\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate to hidden layer 1\n",
    "        dz1 = np.dot(dz2, self.weights_hidden1_hidden2.T) * relu_derivative(self.z1)\n",
    "        dw1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        self.weights_hidden2_output -= self.learning_rate * dw3\n",
    "        self.bias_output -= self.learning_rate * db3\n",
    "\n",
    "        self.weights_hidden1_hidden2 -= self.learning_rate * dw2\n",
    "        self.bias_hidden2 -= self.learning_rate * db2\n",
    "\n",
    "        self.weights_input_hidden1 -= self.learning_rate * dw1\n",
    "        self.bias_hidden1 -= self.learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y, y_pred)\n",
    "            \n",
    "            # Backward pass (backpropagation)\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_size = 14\n",
    "    hidden_layer1_size = 100\n",
    "    hidden_layer2_size = 40\n",
    "    output_size = 4\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "\n",
    "    # Create a neural network\n",
    "    nn = NeuralNetwork(input_size, hidden_layer1_size, hidden_layer2_size, output_size, learning_rate)\n",
    "\n",
    "    # Generate random input data (X) and labels (y) for 5 samples\n",
    "    X = np.random.randn(5, input_size)  # 5 samples, 14 features each\n",
    "    y = np.random.randint(0, 4, size=5)  # Random labels (from 0 to 3)\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_encoded = one_hot_encode(y, output_size)\n",
    "\n",
    "    # Train the neural network\n",
    "    nn.train(X, y_encoded, epochs=epochs)\n",
    "\n",
    "    # Perform a forward pass on the input data to get the predicted probabilities\n",
    "    predictions = nn.forward(X)\n",
    "    \n",
    "    # Convert predictions to Pandas DataFrame for easier visualization (optional)\n",
    "    predictions_df = pd.DataFrame(predictions, columns=[f'Class_{i}' for i in range(output_size)])\n",
    "    \n",
    "    print(\"\\nPredicted Probabilities:\")\n",
    "    print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# ReLU derivative\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data point, weights, and biases from CSV files\n",
    "data_point = np.array([-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1])\n",
    "\n",
    "w0_input_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\a\\w.csv', index_col=0, nrows=14, header=None).values\n",
    "w0_hidden1_hidden2 = pd.read_csv(r'Task_1\\a\\w.csv', index_col=0, skiprows=range(14), nrows=100, usecols=range(41), header=None).values\n",
    "w0_hidden2_output = pd.read_csv(r'Task_1\\a\\w.csv', index_col=0, skiprows=range(114), nrows=40, usecols=range(5), header=None).values\n",
    "\n",
    "b0_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\a\\b.csv', index_col=0, nrows=1, header=None).values\n",
    "b0_hidden2 = pd.read_csv(r'Task_1\\a\\b.csv', index_col=0, skiprows=range(1), nrows=1, usecols=range(41), header=None).values\n",
    "b0_output = pd.read_csv(r'Task_1\\a\\b.csv', index_col=0, skiprows=range(2), nrows=1, usecols=range(5), header=None).values\n",
    "\n",
    "w1_input_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\b\\w-100-40-4.csv', index_col=0, nrows=14, header=None).values\n",
    "w1_hidden1_hidden2 = pd.read_csv(r'Task_1\\b\\w-100-40-4.csv', index_col=0, skiprows=range(14), nrows=100, usecols=range(41), header=None).values\n",
    "w1_hidden2_output = pd.read_csv(r'Task_1\\b\\w-100-40-4.csv', index_col=0, skiprows=range(114), nrows=40, usecols=range(5), header=None).values\n",
    "\n",
    "b1_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\b\\b-100-40-4.csv', index_col=0, nrows=1, header=None).values\n",
    "b1_hidden2 = pd.read_csv(r'Task_1\\b\\b-100-40-4.csv', index_col=0, skiprows=range(1), nrows=1, usecols=range(41), header=None).values\n",
    "b1_output = pd.read_csv(r'Task_1\\b\\b-100-40-4.csv', index_col=0, skiprows=range(2), nrows=1, usecols=range(5), header=None).values\n",
    "\n",
    "correct_grad_w0_input_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\a\\true-dw.csv', nrows=14, header=None).values\n",
    "correct_grad_w0_hidden1_hidden2 = pd.read_csv(r'Task_1\\a\\true-dw.csv', skiprows=range(14), nrows=100, usecols=range(40), header=None).values\n",
    "correct_grad_w0_hidden2_output = pd.read_csv(r'Task_1\\a\\true-dw.csv', skiprows=range(114), nrows=40, usecols=range(4), header=None).values\n",
    "\n",
    "correct_grad_b0_hidden1 = pd.read_csv(\n",
    "    r'Task_1\\a\\true-db.csv', nrows=1, header=None).values\n",
    "correct_grad_b0_hidden2 = pd.read_csv(r'Task_1\\a\\true-db.csv', skiprows=range(1), nrows=1, usecols=range(40), header=None).values\n",
    "correct_grad_b0_output = pd.read_csv(r'Task_1\\a\\true-db.csv', skiprows=range(2), nrows=1, usecols=range(4), header=None).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_grad_w0_input_hidden1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14, 100), (100, 40), (40, 4), (1, 100), (1, 40), (1, 4))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0_input_hidden1.shape, w0_hidden1_hidden2.shape, w0_hidden2_output.shape, b0_hidden1.shape, b0_hidden2.shape, b0_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# ReLU derivative\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # For numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m\n",
    "\n",
    "# Load data point, weights, and biases from CSV files\n",
    "def load_data():\n",
    "    data_point = pd.read_csv('data_point.txt', header=None).values\n",
    "    \n",
    "    W0 = pd.read_csv('Task_1/a/W0.csv', index_col=0).values\n",
    "    b0 = pd.read_csv('Task_1/a/b0.csv', index_col=0).values\n",
    "    \n",
    "    W1 = pd.read_csv('Task_1/b/W1.csv', index_col=0).values\n",
    "    b1 = pd.read_csv('Task_1/b/b1.csv', index_col=0).values\n",
    "    \n",
    "    correct_grad_W0 = pd.read_csv('Task_1/a/grad_W0.csv', header=None).values\n",
    "    correct_grad_b0 = pd.read_csv('Task_1/a/grad_b0.csv', header=None).values\n",
    "    \n",
    "    return data_point, W0, b0, W1, b1, correct_grad_W0, correct_grad_b0\n",
    "\n",
    "# Perform forward pass\n",
    "def forward_pass(X, W0, b0, W1, b1):\n",
    "    # First layer forward pass\n",
    "    z1 = np.dot(X, W0) + b0\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    # Second layer forward pass (output layer with Softmax)\n",
    "    z2 = np.dot(a1, W1) + b1\n",
    "    output = softmax(z2)\n",
    "    \n",
    "    return z1, a1, output\n",
    "\n",
    "# Perform backpropagation to calculate gradients\n",
    "def backward_pass(X, y_true, z1, a1, output, W1):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer gradient (Softmax + Cross-Entropy)\n",
    "    dz2 = output - y_true\n",
    "    dW1 = np.dot(a1.T, dz2) / m\n",
    "    db1 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Hidden layer 1 gradient\n",
    "    dz1 = np.dot(dz2, W1.T) * relu_derivative(z1)\n",
    "    dW0 = np.dot(X.T, dz1) / m\n",
    "    db0 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW0, db0, dW1, db1\n",
    "\n",
    "# Save gradients to CSV files\n",
    "def save_gradients(dW1, db1):\n",
    "    pd.DataFrame(dW1).to_csv('Task_1/b/grad_W1.csv', header=None, index=False)\n",
    "    pd.DataFrame(db1).to_csv('Task_1/b/grad_b1.csv', header=None, index=False)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    X, W0, b0, W1, b1, correct_grad_W0, correct_grad_b0 = load_data()\n",
    "    \n",
    "    # Generate a one-hot encoded label for comparison (since we aren't provided with true labels)\n",
    "    y_true = np.zeros((1, 4))  # Assume it's a classification task with 4 classes\n",
    "    y_true[0, 0] = 1  # Assign label 0 for simplicity\n",
    "\n",
    "    # Forward pass for W0, b0\n",
    "    z1, a1, output = forward_pass(X, W0, b0, W1, b1)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    dW0, db0, dW1, db1 = backward_pass(X, y_true, z1, a1, output, W1)\n",
    "    \n",
    "    # Compare dW0 and db0 with correct gradients (sanity check)\n",
    "    print(\"Gradient Check for W0:\", np.allclose(dW0, correct_grad_W0))\n",
    "    print(\"Gradient Check for b0:\", np.allclose(db0, correct_grad_b0))\n",
    "\n",
    "    # Save the gradients for W1 and b1\n",
    "    save_gradients(dW1, db1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
